{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ff2aba",
   "metadata": {},
   "source": [
    "# Deploy Fine-tuned LLM via SageMaker\n",
    "In this notebook, I will deploy the fine-tuned LLaMA2 model for serving using model artifact from the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0af6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.175.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0909779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::513033806411:role/service-role/AmazonSageMaker-ExecutionRole-20210815T111148\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae3d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.9.3\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import json\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 900\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': '/opt/ml/model',\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu),  # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"<HUGGING_FACE_TOKEN>\"  # Your Hugging Face Hub token\n",
    "  # ,'HF_MODEL_QUANTIZE': \"bitsandbytes\", # Optional: comment in to quantize\n",
    "}\n",
    "\n",
    "# Specify the S3 URI of your model artifact\n",
    "model_data = 's3://sagemaker-us-east-1-513033806411/huggingface-qlora-2024-02-17-17-15-24-2024-02-17-17-16-14-114/output/model.tar.gz'\n",
    "\n",
    "# create HuggingFaceModel with the image uri and model data\n",
    "llm_model = HuggingFaceModel(\n",
    "  model_data=model_data,  # Use your fine-tuned model artifact\n",
    "  role=role,  # IAM role with necessary permissions\n",
    "  image_uri=llm_image,  # LLM image URI\n",
    "  env=config  # Pass the configuration\n",
    ")\n",
    "\n",
    "# Deploy model to an endpoint\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,  # Adjust as necessary\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58f3bd",
   "metadata": {},
   "source": [
    "### Test the model endpoint\n",
    "The model is finished deploying and ready for testing. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e776860c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"\\nJesus Christ on a Harley-Davidson. \\n\\nThat's a good name for a band.\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"### Instruction:\n",
    "Respond to this Reddit post with an award winning top comment.\n",
    "\n",
    "### Reddit Post:\n",
    "Let's go Easter, hell yeah!!\n",
    "\n",
    "### Image Context:\n",
    "- Description: A person dressed as Jesus riding a Harley-Davidson motorcycle with Easter decorations in the background\n",
    "- Text: \n",
    "- Celebrities: \n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "parameters = {\"max_new_tokens\": 64,\n",
    "             \"stop\": [\"</s>\"]}\n",
    "\n",
    "# \"parameters\": {\n",
    "#     \"do_sample\": True,\n",
    "#     \"top_p\": 0.6,\n",
    "#     \"temperature\": 0.9,\n",
    "#     \"top_k\": 50,\n",
    "#     \"max_new_tokens\": 512,\n",
    "#     \"repetition_penalty\": 1.03,\n",
    "#     \"stop\": [\"</s>\"]\n",
    "#   }\n",
    "\n",
    "response = llm.predict({\"inputs\": prompt,\n",
    "                       \"parameters\": parameters})\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
