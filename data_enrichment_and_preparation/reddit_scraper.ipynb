{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb78580f",
   "metadata": {},
   "source": [
    "# Scraping the *r/funny* subreddit with PRAW\n",
    "## 1. Introduction\n",
    "This is the beginning of my project where I will be collecting all of the necessary data to fine-tune my LLM. I will be using the `PRAW` library to scrape posts and collect the following fields:\n",
    "- `post.id`: Used for partition key\n",
    "- `post.title`: Title of the post\n",
    "- `post.selftext`: The body of the post\n",
    "- `post.url`: This is the image or video\n",
    "- `post.created_utc`: Datetime of post\n",
    "- `post.score`: Number of upvotes\n",
    "- `post.num_comments`: Number of comments\n",
    "- `top_comment.body`: Text of the top comment\n",
    "- `top_comment.score`: Number of upvotes for the top comment\n",
    "\n",
    "Because BLIP is an image-to-text model, **I am going to keep the scope of post.urls to .png and .jpg files.** GIFs and videos won't be processed, but instead their descriptions will be inferred using few-shot learning with GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c407c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2e92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import praw\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d2d51",
   "metadata": {},
   "source": [
    "## 2. Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2ce991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    \"\"\"Get secret from AWS Secrets Manager\"\"\"\n",
    "\n",
    "    secret_name = \"reddit_scraper\"\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "\n",
    "    return json.loads(secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ff6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Reddit Client\n",
    "secret = get_secret()\n",
    "client_id = secret['client_id']\n",
    "client_secret = secret['client_secret']\n",
    "password = secret['user_password']\n",
    "user_agent = secret['user_agent']\n",
    "username = secret['username']\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    password=password,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba41c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize s3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'sagemaker-us-east-1-513033806411'\n",
    "\n",
    "# Initialize dynamodb\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('funny-reddit-posts')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b877581",
   "metadata": {},
   "source": [
    "## 3. Scraping Reddit\n",
    "First, I define some helper functions. I am going to store all of the posts in DynamoDB using the `submissionId` as the unique parition key. I will then extract the images from the posts and dump them into my S3 bucket, referencing the object key in the DynamoDB table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3f25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_dynamodb(item):\n",
    "    try:\n",
    "        table.put_item(Item=item)\n",
    "        print(f\"Successfully inserted post: {post.id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting post: {post.id}, Error: {str(e)}\")\n",
    "        \n",
    "    \n",
    "def download_image(image_url, bucket_name, s3_path):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            # Upload directly from the BytesIO object to avoid saving locally\n",
    "            s3.upload_fileobj(BytesIO(response.content), bucket_name, s3_path)\n",
    "            print(f\"Uploaded {image_url} to s3://{bucket_name}/{s3_path}\")\n",
    "            return f\"s3://{bucket_name}/{s3_path}\"\n",
    "        else:\n",
    "            print(f\"Failed to download image {image_url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception during download/upload: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344044a",
   "metadata": {},
   "source": [
    "Next, I define functions to iterate through the posts and store the relevant data in DynamoDB and images in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d9dee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_top_comment(post):\n",
    "    \"\"\"Retreive the most upvoted comment from a post\"\"\"\n",
    "    \n",
    "    # Remove MoreComments objects for a flat comment list\n",
    "    post.comments.replace_more(limit=0)\n",
    "    # Filter out comments by moderators and sort by score\n",
    "    top_comments = [comment for comment in post.comments if not comment.stickied and not comment.distinguished]\n",
    "    if top_comments:\n",
    "        # Sort the comments based on score in descending order and take the top one\n",
    "        top_comment = sorted(top_comments, key=lambda x: x.score, reverse=True)[0]\n",
    "        print(f\"Top Comment: {top_comment.body} (Score: {top_comment.score})\\n\")\n",
    "        return top_comment\n",
    "    else:\n",
    "        print(\"No comments found.\\n\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def scrape_posts(top_posts):\n",
    "    \"\"\"Iterate through the top posts in the subreddit generator\n",
    "       and store them in DynamoDB/S3\"\"\"\n",
    "    \n",
    "    for post in top_posts:\n",
    "    \n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"URL: {post.url}\")\n",
    "    # Check if the post has a body (selftext), and print it if it does\n",
    "    if post.selftext:\n",
    "        print(f\"Body: {post.selftext}\\n\")\n",
    "    else:\n",
    "        print(\"No body for this post.\\n\")\n",
    "\n",
    "    # get top comment\n",
    "    top_comment = get_top_comment(post)  \n",
    "    \n",
    "    item = {\n",
    "            'submissionId': post.id,  # Use post ID as the partition key\n",
    "            'title': post.title,\n",
    "            'body': post.selftext,\n",
    "            'url': post.url,\n",
    "            'createdUtc': int(post.created_utc),\n",
    "            'score': post.score,\n",
    "            'numComments': post.num_comments,\n",
    "            'topComment': top_comment.body if top_comment else \"N/A\",\n",
    "            'topCommentScore': top_comment.score if top_comment else 0\n",
    "            }\n",
    "    \n",
    "    # If the post URL points to an image (.jpg or .png)\n",
    "    if post.url.endswith(('.jpg', '.png')):\n",
    "        s3_path = f\"reddit/funny/posts/{post.id}{post.url[-4:]}\"  \n",
    "        image_s3_url = download_image(post.url, bucket_name, s3_path)\n",
    "\n",
    "        # Add the S3 URL to your item before storing to DynamoDB\n",
    "        item['imageS3Url'] = image_s3_url or \"N/A\"\n",
    "        \n",
    "    \n",
    "    # Insert the item into DynamoDB\n",
    "    write_to_dynamodb(item)\n",
    "\n",
    "    time.sleep(0.6)  # Waits for 0.6 seconds before processing the next post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e281cc",
   "metadata": {},
   "source": [
    "### a) Get Top Posts of All Time\n",
    "In this cell, I collect the top posts of all time from r/funny. The Reddit API limits the amount returned, so this will give ~1,000 posts. The output below is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9569ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: My cab driver tonight was so excited to share with me that heâ€™d made the cover of the calendar. I told him Iâ€™d help let the world see\n",
      "URL: https://i.redd.it/tojcmbvjwk601.jpg\n",
      "No body for this post.\n",
      "\n",
      "Top Comment: He's not just on the cover but also [Mr. December](https://i.imgur.com/J4wQbxf.png)\n",
      "\n",
      "\n",
      "Here's the [whole calendar](https://nyctaxicalendar.com/) which features plenty of shirtless NYC cab dudes.\n",
      " (Score: 26712)\n",
      "\n",
      "Successfully inserted post: 7mjw12\n",
      "Uploaded https://i.redd.it/tojcmbvjwk601.jpg to s3://sagemaker-us-east-1-513033806411/reddit/funny/posts/7mjw12.jpg\n",
      "\n",
      "Title: Guardians of the Front Page\n",
      "URL: http://i.imgur.com/OOFRJvr.gifv\n",
      "No body for this post.\n",
      "\n",
      "Top Comment: Can't wait to upvote this 17 different times later this week. (Score: 26644)\n",
      "\n",
      "Successfully inserted post: 5gn8ru\n",
      "\n",
      "Title: Gas station worker takes precautionary measures after customer refused to put out his cigarette\n",
      "URL: https://gfycat.com/ResponsibleJadedAmericancurl\n",
      "No body for this post.\n",
      "\n",
      "Top Comment: I don't even think the guy smoking would have caught on fire though. He seems extremely retardant.  (Score: 32117)\n",
      "\n",
      "Successfully inserted post: 7431qq\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Define the subreddit you want to scrape\n",
    "subreddit = reddit.subreddit('funny')\n",
    "\n",
    "# Fetch the top posts\n",
    "top_posts = subreddit.top(time_filter = \"all\", limit = None)  # Adjust the limit as needed\n",
    "\n",
    "# scrape top posts of all time\n",
    "scrape_posts(top_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a5d24",
   "metadata": {},
   "source": [
    "### b) Get Top Posts of the Past Year\n",
    "Next, I collect the top posts from the past year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a502a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adam Sandler and Jennifer Aniston are shocked by the size of an Australian reporter\n",
      "URL: https://v.redd.it/o72vkte6dnta1\n",
      "No body for this post.\n",
      "\n",
      "Top Comment: \"Put your hat on\", that's hilarious (Score: 13782)\n",
      "\n",
      "Successfully inserted post: 12knt5j\n",
      "\n",
      "Title: My hometown just unveiled a 9/11 memorial at the fireman's museum. Think they could have used another set of eyes on this one...\n",
      "URL: https://i.imgur.com/Y8BzrdR.jpg\n",
      "No body for this post.\n",
      "\n",
      "Top Comment: That's...unfortunate. (Score: 18403)\n",
      "\n",
      "Successfully inserted post: 11qhab2\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Fetch the top posts\n",
    "top_posts = subreddit.top(time_filter = \"year\", limit = None)  # Adjust the limit as needed\n",
    "\n",
    "# scrape top posts of all time\n",
    "scrape_posts(top_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86b8f2",
   "metadata": {},
   "source": [
    "### c) Get Top Posts of the Past Month\n",
    "Finally, I narrow the scope to the past month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1418289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: my favorite video so far this year\n",
      "URL: https://v.redd.it/38ktcvgthubc1\n",
      "No body for this post.\n",
      "\n",
      "Top Comment: My man is just playing the hand he was dealt. (Score: 11975)\n",
      "\n",
      "Successfully inserted post: 19470zf\n",
      "\n",
      "Title: London, UK\n",
      "URL: https://i.redd.it/fa536c2dxfcc1.png\n",
      "No body for this post.\n",
      "\n",
      "Top Comment: We love the british tourist in amsterdam.... such good behaviourðŸ˜ (Score: 5902)\n",
      "\n",
      "Successfully inserted post: 196l27v\n",
      "Uploaded https://i.redd.it/fa536c2dxfcc1.png to s3://sagemaker-us-east-1-513033806411/reddit/funny/posts/196l27v.png\n",
      "\n",
      "Title: My coworker was asked to cut the cake today at work.\n",
      "URL: https://i.redd.it/dhs2xa401bfc1.jpeg\n",
      "No body for this post.\n",
      "\n",
      "Top Comment: That is a person ensuring they never get asked to cut a cake again. (Score: 18277)\n",
      "\n",
      "Successfully inserted post: 1adm9h7\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Fetch the top posts\n",
    "top_posts = subreddit.top(time_filter = \"month\", limit = None)  # Adjust the limit as needed\n",
    "\n",
    "# scrape top posts of all time\n",
    "scrape_posts(top_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04c0ea",
   "metadata": {},
   "source": [
    "### d) Bypass API Limits using Random Search\n",
    "The target is 2,500 posts and I am a few hundred short of that, so in order to collect more top posts, I am going to randomly search top posts from a corpus of text. This mitigates the cap of 1,000 posts that the Reddit API enforces. To make sure that I have a good distribution, I will limit posts collected for each search word to three. I also set a minimum of 25k upvotes to ensure quality of the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f29d7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'Adventure':\n",
      "Title: Our dog who ran off on an adventure for 7.5 hours ringing our doorbell at 3 am to let us know sheâ€™s home | Score: 153599\n",
      "Top Comment: Dogs can be the most inconsiderate roommates. (Score: 12632)\n",
      "\n",
      "Successfully inserted post: o9kpkg\n",
      "\n",
      "Title: The Adventures of Asian Superman | Score: 90298\n",
      "Top Comment: Perry could learn a thing or two about cultural sensitivity from [Lois Lane](https://www.comicbookdaily.com/wp-content/uploads/2009/10/Lois-Lane-106.jpg). (Score: 5696)\n",
      "\n",
      "Successfully inserted post: jqwv4h\n",
      "Uploaded https://i.redd.it/u2c6so4xl7y51.png to s3://sagemaker-us-east-1-513033806411/reddit/funny/posts/jqwv4h.png\n",
      "\n",
      "Title: What an adventure! | Score: 73435\n",
      "Top Comment: Meanwhile, thousands of documents on floor 4 go unstapled.  You monster. (Score: 10858)\n",
      "\n",
      "Successfully inserted post: 6586ox\n",
      "Uploaded https://i.redd.it/6zdmyk0cudry.jpg to s3://sagemaker-us-east-1-513033806411/reddit/funny/posts/6586ox.jpg\n"
     ]
    }
   ],
   "source": [
    "# keep track of posts already collected\n",
    "post_ids = []\n",
    "\n",
    "# Define the subreddit you want to scrape\n",
    "subreddit = reddit.subreddit('funny')\n",
    "\n",
    "# Iterate through random words and time filters\n",
    "for word in search_words:\n",
    "    print(f\"Searching for '{word}':\")\n",
    "    # Conduct search\n",
    "    search_results = subreddit.search(query=word, sort='top', time_filter=\"all\", limit=3)\n",
    "    for post in search_results:\n",
    "        print(f\"Title: {post.title} | Score: {post.score}\")\n",
    "\n",
    "        # store submission if > 25k upvotes\n",
    "        if post.score > 25000:\n",
    "\n",
    "            # don't reprocess already seen posts\n",
    "            if post.id in post_ids:\n",
    "                continue\n",
    "            else:\n",
    "                # get top comment\n",
    "                top_comment = get_top_comment(post)  \n",
    "                item = {\n",
    "                'submissionId': post.id,  # Use post ID as the primary key\n",
    "                'title': post.title,\n",
    "                'body': post.selftext,\n",
    "                'url': post.url,\n",
    "                'createdUtc': int(post.created_utc),\n",
    "                'score': post.score,\n",
    "                'numComments': post.num_comments,\n",
    "                'topComment': top_comment.body if top_comment else \"N/A\",\n",
    "                'topCommentScore': top_comment.score if top_comment else 0\n",
    "                }\n",
    "\n",
    "                # add to processed posts list\n",
    "                post_ids.append(post.id)\n",
    "\n",
    "                # If the post URL points to an image (.jpg or .png)\n",
    "                if post.url.endswith(('.jpg', '.png')):\n",
    "                    s3_path = f\"reddit/funny/posts/{post.id}{post.url[-4:]}\"  \n",
    "                    image_s3_url = download_image(post.url, bucket_name, s3_path)\n",
    "\n",
    "                    # Add the S3 URL to your item before storing to DynamoDB\n",
    "                    item['imageS3Url'] = image_s3_url or \"N/A\"\n",
    "                    \n",
    "                # Insert the item into DynamoDB\n",
    "                write_to_dynamodb(item)\n",
    "\n",
    "        # Ensure to respect Reddit's rate limits\n",
    "        time.sleep(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c48bf",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "This concludes the first section of this project. I now have ~2,700 posts stored in DynamoDB for easy access, along with the images in S3. In the next notebook, I will begin processing these images to generate captions to complete the dataset for fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
